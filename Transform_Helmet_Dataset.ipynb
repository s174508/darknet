{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e37e039",
   "metadata": {},
   "source": [
    "Transforms the HELMET data set such that darknet can use it for training\n",
    "\n",
    "for `classes`, choose between one of the three possible configurations (see the class dictionaries further down)\n",
    "\n",
    "`original_dir` is the path to the helmet data folder, from which the folders `annotation` and `image` are visible. Annotations and all image parts must be unzipped beforehand\n",
    "\n",
    "`target_dir` is the path to the folder where you want the transformed data set. Haven't tested if `target_dir` = `original_dir` works. If you want to try then perhaps outcomment the two calls to `shutil.copy2` since the images don't need to be copied\n",
    "\n",
    "`hpc_dir` is the path to the location where the transformed data will be placed on HPC. This must be accurate or darknet won't be able to find the images. In case of a mistake, doing \"find and replace\" on `train.txt`, `test.txt` and `validation.txt` should remedy the issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b1ea102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from collections import OrderedDict\n",
    "import shutil\n",
    "import zipfile\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2412bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = \"singleclass\" #choose singleclass / multiclass / fullclass\n",
    "\n",
    "original_dir = \"D:/Data/Helmet_Dataset/\"\n",
    "target_dir = \"D:/Data/Helmet_Dataset_\" + classes + \"/\"\n",
    "hpc_dir = \"/work3/s174508/Helmet_Dataset_\" + classes + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d06e99b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_classes = None\n",
    "\n",
    "if classes == \"singleclass\":\n",
    "    dict_classes = {\n",
    "    \"DHelmet\":0,\n",
    "    \"DNoHelmet\":0,\n",
    "    \"DHelmetP0Helmet\":0,\n",
    "    \"DHelmetP0NoHelmet\":0,\n",
    "    \"DNoHelmetP0NoHelmet\":0,\n",
    "    \"DHelmetP0HelmetP1Helmet\":0,\n",
    "    \"DHelmetP0NoHelmetP1Helmet\":0,\n",
    "    \"DHelmetP0NoHelmetP1NoHelmet\":0,\n",
    "    \"DNoHelmetP0HelmetP1NoHelmet\":0,\n",
    "    \"DNoHelmetP0NoHelmetP1Helmet\":0,\n",
    "    \"DNoHelmetP0NoHelmetP1NoHelmet\":0,\n",
    "    \"DHelmetP0HelmetP1HelmetP2Helmet\":0,\n",
    "    \"DHelmetP0HelmetP1NoHelmetP2Helmet\":0,\n",
    "    \"DHelmetP0HelmetP1NoHelmetP2NoHelmet\":0,\n",
    "    \"DHelmetP0NoHelmetP1HelmetP2Helmet\":0,\n",
    "    \"DHelmetP0NoHelmetP1NoHelmetP2Helmet\":0,\n",
    "    \"DHelmetP0NoHelmetP1NoHelmetP2NoHelmet\":0,\n",
    "    \"DNoHelmetP0NoHelmetP1NoHelmetP2Helmet\":0,\n",
    "    \"DNoHelmetP0NoHelmetP1NoHelmetP2NoHelmet\":0,\n",
    "    \"DHelmetP0NoHelmetP1NoHelmetP2NoHelmetP3Helmet\":0,\n",
    "    \"DHelmetP0NoHelmetP1NoHelmetP2NoHelmetP3NoHelmet\":0,\n",
    "    \"DNoHelmetP0NoHelmetP1NoHelmetP2NoHelmetP3NoHelmet\":0,\n",
    "    \"DHelmetP1Helmet\":0,\n",
    "    \"DHelmetP1NoHelmet\":0,\n",
    "    \"DNoHelmetP1Helmet\":0,\n",
    "    \"DNoHelmetP1NoHelmet\":0,\n",
    "    \"DHelmetP1HelmetP2Helmet\":0,\n",
    "    \"DHelmetP1HelmetP2NoHelmet\":0,\n",
    "    \"DHelmetP1NoHelmetP2Helmet\":0,\n",
    "    \"DHelmetP1NoHelmetP2NoHelmet\":0,\n",
    "    \"DNoHelmetP1HelmetP2Helmet\":0,\n",
    "    \"DNoHelmetP1NoHelmetP2Helmet\":0,\n",
    "    \"DNoHelmetP1NoHelmetP2NoHelmet\":0,\n",
    "    \"DHelmetP1NoHelmetP2NoHelmetP3Helmet\":0,\n",
    "    \"DHelmetP1NoHelmetP2NoHelmetP3NoHelmet\":0,\n",
    "    \"DNoHelmetP1NoHelmetP2NoHelmetP3NoHelmet\":0\n",
    "    }\n",
    "elif classes == \"multiclass\":\n",
    "    dict_classes = {\n",
    "    \"DHelmet\":0,\n",
    "    \"DNoHelmet\":0,\n",
    "    \"DHelmetP0Helmet\":1,\n",
    "    \"DHelmetP0NoHelmet\":1,\n",
    "    \"DNoHelmetP0NoHelmet\":1,\n",
    "    \"DHelmetP0HelmetP1Helmet\":2,\n",
    "    \"DHelmetP0NoHelmetP1Helmet\":2,\n",
    "    \"DHelmetP0NoHelmetP1NoHelmet\":2,\n",
    "    \"DNoHelmetP0HelmetP1NoHelmet\":2,\n",
    "    \"DNoHelmetP0NoHelmetP1Helmet\":2,\n",
    "    \"DNoHelmetP0NoHelmetP1NoHelmet\":2,\n",
    "    \"DHelmetP0HelmetP1HelmetP2Helmet\":3,\n",
    "    \"DHelmetP0HelmetP1NoHelmetP2Helmet\":3,\n",
    "    \"DHelmetP0HelmetP1NoHelmetP2NoHelmet\":3,\n",
    "    \"DHelmetP0NoHelmetP1HelmetP2Helmet\":3,\n",
    "    \"DHelmetP0NoHelmetP1NoHelmetP2Helmet\":3,\n",
    "    \"DHelmetP0NoHelmetP1NoHelmetP2NoHelmet\":3,\n",
    "    \"DNoHelmetP0NoHelmetP1NoHelmetP2Helmet\":3,\n",
    "    \"DNoHelmetP0NoHelmetP1NoHelmetP2NoHelmet\":3,\n",
    "    \"DHelmetP0NoHelmetP1NoHelmetP2NoHelmetP3Helmet\":4,\n",
    "    \"DHelmetP0NoHelmetP1NoHelmetP2NoHelmetP3NoHelmet\":4,\n",
    "    \"DNoHelmetP0NoHelmetP1NoHelmetP2NoHelmetP3NoHelmet\":4,\n",
    "    \"DHelmetP1Helmet\":5,\n",
    "    \"DHelmetP1NoHelmet\":5,\n",
    "    \"DNoHelmetP1Helmet\":5,\n",
    "    \"DNoHelmetP1NoHelmet\":5,\n",
    "    \"DHelmetP1HelmetP2Helmet\":6,\n",
    "    \"DHelmetP1HelmetP2NoHelmet\":6,\n",
    "    \"DHelmetP1NoHelmetP2Helmet\":6,\n",
    "    \"DHelmetP1NoHelmetP2NoHelmet\":6,\n",
    "    \"DNoHelmetP1HelmetP2Helmet\":6,\n",
    "    \"DNoHelmetP1NoHelmetP2Helmet\":6,\n",
    "    \"DNoHelmetP1NoHelmetP2NoHelmet\":6,\n",
    "    \"DHelmetP1NoHelmetP2NoHelmetP3Helmet\":7,\n",
    "    \"DHelmetP1NoHelmetP2NoHelmetP3NoHelmet\":7,\n",
    "    \"DNoHelmetP1NoHelmetP2NoHelmetP3NoHelmet\":7\n",
    "    }\n",
    "elif classes == \"fullclass\":\n",
    "    dict_classes = {\n",
    "    \"DHelmet\":0,\n",
    "    \"DNoHelmet\":1,\n",
    "    \"DHelmetP0Helmet\":2,\n",
    "    \"DHelmetP0NoHelmet\":3,\n",
    "    \"DNoHelmetP0NoHelmet\":4,\n",
    "    \"DHelmetP0HelmetP1Helmet\":5,\n",
    "    \"DHelmetP0NoHelmetP1Helmet\":6,\n",
    "    \"DHelmetP0NoHelmetP1NoHelmet\":7,\n",
    "    \"DNoHelmetP0HelmetP1NoHelmet\":8,\n",
    "    \"DNoHelmetP0NoHelmetP1Helmet\":9,\n",
    "    \"DNoHelmetP0NoHelmetP1NoHelmet\":10,\n",
    "    \"DHelmetP0HelmetP1HelmetP2Helmet\":11,\n",
    "    \"DHelmetP0HelmetP1NoHelmetP2Helmet\":12,\n",
    "    \"DHelmetP0HelmetP1NoHelmetP2NoHelmet\":13,\n",
    "    \"DHelmetP0NoHelmetP1HelmetP2Helmet\":14,\n",
    "    \"DHelmetP0NoHelmetP1NoHelmetP2Helmet\":15,\n",
    "    \"DHelmetP0NoHelmetP1NoHelmetP2NoHelmet\":16,\n",
    "    \"DNoHelmetP0NoHelmetP1NoHelmetP2Helmet\":17,\n",
    "    \"DNoHelmetP0NoHelmetP1NoHelmetP2NoHelmet\":18,\n",
    "    \"DHelmetP0NoHelmetP1NoHelmetP2NoHelmetP3Helmet\":19,\n",
    "    \"DHelmetP0NoHelmetP1NoHelmetP2NoHelmetP3NoHelmet\":20,\n",
    "    \"DNoHelmetP0NoHelmetP1NoHelmetP2NoHelmetP3NoHelmet\":21,\n",
    "    \"DHelmetP1Helmet\":22,\n",
    "    \"DHelmetP1NoHelmet\":23,\n",
    "    \"DNoHelmetP1Helmet\":24,\n",
    "    \"DNoHelmetP1NoHelmet\":25,\n",
    "    \"DHelmetP1HelmetP2Helmet\":26,\n",
    "    \"DHelmetP1HelmetP2NoHelmet\":27,\n",
    "    \"DHelmetP1NoHelmetP2Helmet\":28,\n",
    "    \"DHelmetP1NoHelmetP2NoHelmet\":29,\n",
    "    \"DNoHelmetP1HelmetP2Helmet\":30,\n",
    "    \"DNoHelmetP1NoHelmetP2Helmet\":31,\n",
    "    \"DNoHelmetP1NoHelmetP2NoHelmet\":32,\n",
    "    \"DHelmetP1NoHelmetP2NoHelmetP3Helmet\":33,\n",
    "    \"DHelmetP1NoHelmetP2NoHelmetP3NoHelmet\":34,\n",
    "    \"DNoHelmetP1NoHelmetP2NoHelmetP3NoHelmet\":35\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9abecf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subpath(folder_name):\n",
    "    for i in range(1,8):\n",
    "        if os.path.isdir(original_dir + \"image/part_\" + str(i) + \"/\" + folder_name):\n",
    "            return \"part_\" + str(i) + \"/\" + folder_name + \"/\"\n",
    "    raise NotADirectoryError(\"No directory matching \" + folder_name)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8734c492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_annotation_files(subpath, folder_name):\n",
    "    \n",
    "    files = [None for i in range(100)]\n",
    "    \n",
    "    csv_org_dir = original_dir + \"annotation/\"\n",
    "    csv_file = open(csv_org_dir + folder_name + \".csv\")\n",
    "    annotations = csv.DictReader(csv_file)\n",
    "    \n",
    "    \n",
    "    for row in annotations:\n",
    "        frame = int(row[\"frame_id\"])\n",
    "        \n",
    "        if not files[frame-1]:\n",
    "            if frame < 10:\n",
    "                files[frame-1] = open(\"{}{}0{}.txt\".format(target_dir, subpath, frame), \"w\")\n",
    "            else:\n",
    "                files[frame-1] = open(\"{}{}{}.txt\".format(target_dir, subpath, frame), \"w\")\n",
    "        \n",
    "        label = dict_classes[row[\"label\"]]\n",
    "        x = (int(row[\"x\"]) + int(row[\"w\"])/2) / 1920\n",
    "        y = (int(row[\"y\"]) + int(row[\"h\"])/2) / 1080\n",
    "        w = int(row[\"w\"]) / 1920\n",
    "        h = int(row[\"h\"]) / 1080\n",
    "        files[frame-1].write(\"{} {} {} {} {}\\n\".format(label, x, y, w, h))\n",
    "    \n",
    "    csv_file.close()\n",
    "    for f in files:\n",
    "        if f:\n",
    "            f.close()\n",
    "    \n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "771359b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data():\n",
    "    data_split = open(original_dir + \"data_split.csv\", \"r\")\n",
    "    os.mkdir(target_dir)\n",
    "    for i in range(1, 8):\n",
    "        os.mkdir(target_dir + \"part_\" + str(i))\n",
    "    train = open(target_dir + \"train.txt\", \"w\")\n",
    "    test = open(target_dir + \"test.txt\", \"w\")\n",
    "    validation = open(target_dir + \"validation.txt\", \"w\")\n",
    "    \n",
    "    header = True\n",
    "    \n",
    "    for line in data_split:\n",
    "        if header:\n",
    "            header = False\n",
    "        else:\n",
    "            split_info = line.split(\",\")\n",
    "            folder_name = split_info[0]\n",
    "            folder_set = split_info[1]\n",
    "            subpath = get_subpath(folder_name) # part_x/folder_name/\n",
    "            \n",
    "            # Make annotation files\n",
    "            os.mkdir(target_dir + subpath)\n",
    "            files = make_annotation_files(subpath, folder_name)\n",
    "            \n",
    "            # Copy relevant image files and make train, test and validation files\n",
    "            for i in range(1, 10):\n",
    "                if files[i-1]:\n",
    "                    image_name = \"0\" + str(i) + \".jpg\"\n",
    "                    \n",
    "                    shutil.copy2(original_dir + \"image/\" + subpath + image_name, target_dir + subpath + image_name)\n",
    "                    \n",
    "                    if folder_set == \"training\\n\":\n",
    "                        train.write(hpc_dir + subpath + image_name + \"\\n\")\n",
    "                    elif folder_set == \"test\\n\":\n",
    "                        test.write(hpc_dir + subpath + image_name + \"\\n\")\n",
    "                    if folder_set == \"validation\\n\":\n",
    "                        validation.write(hpc_dir + subpath + image_name + \"\\n\")\n",
    "            \n",
    "            for i in range(10, 101):\n",
    "                if files[i-1]:\n",
    "                    image_name = str(i) + \".jpg\"\n",
    "                    \n",
    "                    shutil.copy2(original_dir + \"image/\" + subpath + image_name, target_dir + subpath + image_name)\n",
    "                    \n",
    "                    if folder_set == \"training\\n\":\n",
    "                        train.write(hpc_dir + subpath + image_name + \"\\n\")\n",
    "                    elif folder_set == \"test\\n\":\n",
    "                        test.write(hpc_dir + subpath + image_name + \"\\n\")\n",
    "                    if folder_set == \"validation\\n\":\n",
    "                        validation.write(hpc_dir + subpath + image_name + \"\\n\")\n",
    "    \n",
    "    data_split.close()\n",
    "    train.close()\n",
    "    test.close()\n",
    "    validation.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b6245c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
